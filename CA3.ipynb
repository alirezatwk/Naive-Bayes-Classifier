{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "به نام خدا\n",
    "\n",
    "پروژه‌ی سوم\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "علیرضا توکلی\n",
    "810197686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# هدف پروژه\n",
    "در این پروژه قصد داریم با استفاده از الگوریتم بیز به تجزیه و تحلیل کامنت‌ها بپردازیم و مثبت یا منفی بودن نظرات را طبق متن آن‌ها تشخیص دهیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# توضیح کلی پروژه\n",
    "در این پروژه می‌خواهیم با توجه به متن و عنوان کامنت‌ها، تشخیص دهیم که نویسنده‌ی آن، کالا را پیش‌نهاد کرده یا نه.\n",
    "برای این کار از الگوریتم گفته شده استفاده خواهیم کرد. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# دیتاهای لود شده\n",
    "دو مجموعه داده در این راه حل لود می‌شود که هر دو مجموعه داده‌های دیجی‌کالا هستند و در فرمت سی‌اس‌وی می‌باشند. عنوان نظرات، خود نظر و مثبت بودن یا نبودن نظر در هر سطر داده‌ها آمده است. مجموعه داده‌های اول برای آموزش مدل ما به کار می‌آید و مجموعه‌ی دیگر برای تست عمل‌کرد مدل خواهد بود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = False\n",
    "smoothingComponent = 10\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('comment_train.csv')\n",
    "test = pd.read_csv('comment_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# فاز اول"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## توضیح این فاز\n",
    "در این فاز به مرتب کردن داده‌های متنی می‌پردازیم و روش‌های مختلف را امتحان می‌کنیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(data, function, stopwords = False, normalize = True):\n",
    "    n, m = data.shape\n",
    "    normalizer = Normalizer()\n",
    "    for i in range(n):\n",
    "        #print(data.iloc[i, 0])\n",
    "        if normalize:\n",
    "            data.iloc[i, 0] = normalizer.normalize(data.iloc[i, 0])\n",
    "            data.iloc[i, 1] = normalizer.normalize(data.iloc[i, 1])\n",
    "        data.iloc[i, 0] = list(map(function, \\\n",
    "                                   word_tokenize(data.iloc[i, 0])))\n",
    "        if stopwords:\n",
    "            for x in data.iloc[i, 0]:\n",
    "                if x in stopwords_list():\n",
    "                    data.iloc[i, 0].remove(x)\n",
    "        data.iloc[i, 1] = list(map(function, \\\n",
    "                                   word_tokenize(data.iloc[i, 1])))\n",
    "        if stopwords:\n",
    "            for x in data.iloc[i, 1]:\n",
    "                if x in stopwords_list():\n",
    "                    data.iloc[i, 1].remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[زیبا, اما, کم, دوام]</td>\n",
       "      <td>[با, وجود, سابقه, خوبی, که, از, برند, ایرانی, ...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[بسیار, عالی]</td>\n",
       "      <td>[بسیار, عالی]</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[سلام]</td>\n",
       "      <td>[من, الان, ۳, هفته, هست, استفاده, میکنم, برای,...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[به, درد, نمیخورهههه]</td>\n",
       "      <td>[عمرش, کمه, تا, یه, هفته, بیشتر, نمیشه, استفاد...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[کلمن, آب]</td>\n",
       "      <td>[فکر, کنین, کلمن, بخرین, با, ذوق, ., کلی, پولش...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                            comment  \\\n",
       "0  [زیبا, اما, کم, دوام]  [با, وجود, سابقه, خوبی, که, از, برند, ایرانی, ...   \n",
       "1          [بسیار, عالی]                                      [بسیار, عالی]   \n",
       "2                 [سلام]  [من, الان, ۳, هفته, هست, استفاده, میکنم, برای,...   \n",
       "3  [به, درد, نمیخورهههه]  [عمرش, کمه, تا, یه, هفته, بیشتر, نمیشه, استفاد...   \n",
       "4             [کلمن, آب]  [فکر, کنین, کلمن, بخرین, با, ذوق, ., کلی, پولش...   \n",
       "\n",
       "         recommend  \n",
       "0  not_recommended  \n",
       "1      recommended  \n",
       "2  not_recommended  \n",
       "3  not_recommended  \n",
       "4  not_recommended  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n",
    "normalizeDataset(train, dummy, stopWords, normalize=normalize)\n",
    "#normalizeDataset(train, lemmatizer.lemmatize, stopWords)\n",
    "#normalizeDataset(train, stemmer.stem, stopWords)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>recommend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[وری, گود]</td>\n",
       "      <td>[تازه, خریدم, یه, مدت, کار, بکنه, مشخص, میشه, ...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[زیاد, مناسب, نیست, رنگ, پس, میده, یه, وقتایی,...</td>\n",
       "      <td>[با, این, قیمت, گزینه‌های, بهتری, هم, میشه, گر...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[پنکه, گوشی]</td>\n",
       "      <td>[خیلی, عالیه, ،, فقط, کاش, از, اون, سمتش, میشد...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[دستگاه, خیلی, ضعیف]</td>\n",
       "      <td>[من, این, فیس, براس, چند, روز, یپش, به, دستم, ...</td>\n",
       "      <td>not_recommended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[عالی, و, بیست]</td>\n",
       "      <td>[بنده, یه, هارد, اکسترنال, دارم, که, کابل, فاب...</td>\n",
       "      <td>recommended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                         [وری, گود]   \n",
       "1  [زیاد, مناسب, نیست, رنگ, پس, میده, یه, وقتایی,...   \n",
       "2                                       [پنکه, گوشی]   \n",
       "3                               [دستگاه, خیلی, ضعیف]   \n",
       "4                                    [عالی, و, بیست]   \n",
       "\n",
       "                                             comment        recommend  \n",
       "0  [تازه, خریدم, یه, مدت, کار, بکنه, مشخص, میشه, ...      recommended  \n",
       "1  [با, این, قیمت, گزینه‌های, بهتری, هم, میشه, گر...  not_recommended  \n",
       "2  [خیلی, عالیه, ،, فقط, کاش, از, اون, سمتش, میشد...      recommended  \n",
       "3  [من, این, فیس, براس, چند, روز, یپش, به, دستم, ...  not_recommended  \n",
       "4  [بنده, یه, هارد, اکسترنال, دارم, که, کابل, فاب...      recommended  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizeDataset(test, dummy, stopWords, normalize=normalize)\n",
    "#normalizeDataset(test, lemmatizer.lemmatize, stopWords)\n",
    "#normalizeDataset(test, stemmer.stem, stopWords)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "تفاوت استیمینگ با لمیتیزشن:\n",
    "\n",
    "در استیمینگ قسمت‌های اضافی ابتدا و انتهای کلمه، مثل ها، ام، اید و ... حذف می‌شوند.\n",
    "\n",
    "اما در لمیتیزشن، کلمه به فرم ریشه‌ی واقعی‌اش در می‌آید. مثلا می‌روم تبدیل به رو می‌شود.\n",
    "\n",
    "اما تاثیر هر روش:\n",
    "\n",
    "| Algorithm | F1\n",
    "| --- | --- |\n",
    "| None | 0.883048620236531\n",
    "| Lemmatizer | 0.8818897637795275\n",
    "| Stemmer | 0.8748353096179183\n",
    "\n",
    "نتایج نشان می‌دهد که اگر از هیچ یک از الگوریتم‌های هضم(به جز نرمالیزشن) استفاده نکنیم، نتیجه‌ی ما به مقدار خیلی کمی بهتر خواهد بود. دلیل آن نیز می‌تواند پیاده‌سازی بد توابع استیم و لمتازر باشد که مثلا در کلمه‌ی دوام، م را جزئی از کلمه نمی‌داند و آن را حذف می‌کند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## نکته‌ی مهم در رابطه با استاپ ورد‌ها\n",
    "\n",
    "استاپ ورد‌ها علاوه بر کند کردن قطعه کدمان، بسیار نتیجه را بد می‌کنند لذا در نتایج ما، استاپ ورد‌ها را اعمال نمی‌کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# فاز دوم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## توضیح این فاز\n",
    "در این فاز به بررسی نحوه‌ی محاسبه‌ی احتمال هر دسته می‌پردازیم. از فرمول بیض استفاده می‌کنیم و برای رفع مشکل مطرح شده از اسموفینگ نیز بهره می‌بریم که توضیح بیش‌تر آن در ادامه آمده است."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "احتمال این که با توجه به مشاهدات ما(که تعداد نمونه‌هایی با کلمه‌ی ایکس در این کامنت است) این نمونه از کلاس سی باشد، را\n",
    "\n",
    "posterior\n",
    "\n",
    "می‌گویند و توسط فرض اضافه‌ی الگوریتم نیو بیض که ویژگی‌ها را مستقل از هم می‌داند، محاسبه می‌شود. در مسئله‌ی ما ایکس هر کلمه‌ای و سی می‌تواند پیشنهاد شده یا نشده باشد. اگر این احتمال را به ازای همه‌ی کلمات داشته باشیم، کار تمام است.\n",
    "\n",
    "احتمال این که بین همه‌ی نمونه‌هایی که عضو دسته‌ی سی هستند، ویژگی ایکس(تعداد کلمات آمده در کامنت) را داشته باشند،\n",
    "\n",
    "likelihood\n",
    "\n",
    "نام دارد که ما برای این که تخمینی از این احتمال داشته باشیم، روی تمامی داده‌های آموزشیمان که در کلاس سی هستند فور زده و تعداد آن‌هایی که ویژگی ایکس(این کلمه‌ی به‌خصوص) را دارند تقسیم بر تعداد کل می‌کنیم.\n",
    "\n",
    "احتمال این که یک داده از دسته‌ی سی باشد را\n",
    "\n",
    "Prior\n",
    "\n",
    "می‌گوییم. به سادگی بر روی تمام نمونه‌های آموزشی حرکت کرده و تعداد اعضای هر کلاس را می‌شماریم. حال احتمال پریور یک کلاس برابر با تعداد آن کلاس تقسیم بر کل داده‌ها می‌شود. در مسئله‌ی ما احتمال این که یک فرد یک محصول را پیشنهاد بدهد یا ندهد است.\n",
    "\n",
    "احتمال آخر که \n",
    "\n",
    "Evidence\n",
    "\n",
    "نام دارد، احتمال این است که یک ویژگی در کل داده‌ها وجود داشته باشد. می‌توانیم برای تخمین آن فور بزنیم روی تمامی داده‌ها و آن‌هایی که این ویژگی را دارند بشماریم و تقسیم بر کل داده‌ها کنیم.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "countDic = [{}, {}]\n",
    "sumAll = [0, 0]\n",
    "n, m = train.shape\n",
    "for i in range(n):\n",
    "    id = 0\n",
    "    if train.iloc[i, 2] == 'recommended':\n",
    "        id = 1\n",
    "    for col in range(2):\n",
    "        for word in train.iloc[i, col]:\n",
    "            words.add(word)\n",
    "            sumAll[id] += 1\n",
    "            if word in countDic[id]:\n",
    "                countDic[id][word] += 1\n",
    "            else:\n",
    "                countDic[id][word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "زیرا در صورتی که کلمه‌ی ایکس‌یک فقط در کلاس سی‌یک آمده باشد، \n",
    "\n",
    "P(c|X) = P(c) * mul(P(x|c))\n",
    "\n",
    "که به دلیل وجود کلمه‌ی ایکس یک در یکی از دسته‌ها داریم\n",
    "\n",
    "P(x1|c1) != 0, P(x1|~c1) = 0\n",
    "\n",
    "پس حتما\n",
    "\n",
    "P(~c1|X) = 0 \n",
    "\n",
    "خواهد شد. پس حتما دسته‌ی سی یک انتخاب می‌شود. زیرا دسته‌ی دیگر مقدار احتمالش صفر شده است."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "در این روش ما متغیری به نام آلفا داریم که به آن اسموفینگ کامپوننت می‌گویند. این متغیر در صورت و مخرج کسر احتمالاتی ما به صورت جمع آمده و باعث می‌شود کسر ۰ نداشته باشیم. این روش پیاده‌سازی خاصی نیز ندارد و فقط کافی است در هنگام محاسبه کردن احتمال‌ها، مقدار از پیش تعیین شده را به صورت و مخرج کسرمان اضافه کنیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# فاز سوم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## توضیح این فاز\n",
    "در این فاز به بررسی روش‌های نوشته شده و نحوه‌ی محاسبه‌ی دقت می‌پردازیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAndPrint(smoothing = True):\n",
    "    n, m = test.shape\n",
    "    \n",
    "    correctDetectedRecommended = 0\n",
    "    correctDetectedNotRecommended = 0\n",
    "    allDetectedRecommended = 0\n",
    "    totalRecommended = 0\n",
    "    wrongcnt = 0\n",
    "    print(\"5 Wrong samples: \")\n",
    "    for i in range(n):\n",
    "        #print(i, test.iloc[i, 0])\n",
    "        prob = []\n",
    "        for id in range(2):\n",
    "            p = 1\n",
    "            for key in test.iloc[i, 0]:\n",
    "                if key in countDic[id]:\n",
    "                    p *= (countDic[id][key] + smoothing * smoothingComponent)\\\n",
    "                            / (sumAll[id] + smoothing * smoothingComponent)\n",
    "                else:\n",
    "                    p *= smoothing * smoothingComponent / (sumAll[id] + smoothing * smoothingComponent)\n",
    "            for key in test.iloc[i, 1]:\n",
    "                if key in countDic[id]:\n",
    "                    p *= (countDic[id][key] + smoothing * smoothingComponent)\\\n",
    "                            / (sumAll[id] + smoothing * smoothingComponent)\n",
    "                else:\n",
    "                    p *= smoothing * smoothingComponent / (sumAll[id] + smoothing * smoothingComponent)\n",
    "            prob.append(p * sumAll[id] / (sumAll[0] + sumAll[1]))\n",
    "        if prob[0] < prob[1]:\n",
    "            allDetectedRecommended += 1\n",
    "            if test.iloc[i, 2] == 'recommended':\n",
    "                correctDetectedRecommended += 1\n",
    "            elif wrongcnt < 5:\n",
    "                print(test.iloc[i])\n",
    "                wrongcnt += 1\n",
    "        elif test.iloc[i, 2] == 'not_recommended':\n",
    "            correctDetectedNotRecommended += 1\n",
    "        elif test.iloc[i, 2] == 'recommended' and wrongcnt < 5:\n",
    "            print(test.iloc[i])\n",
    "            wrongcnt += 1\n",
    "        if test.iloc[i, 2] == 'recommended':\n",
    "            totalRecommended += 1\n",
    "            \n",
    "    accuracy = (correctDetectedNotRecommended + correctDetectedRecommended) / n\n",
    "    precision = correctDetectedRecommended / allDetectedRecommended\n",
    "    recall = correctDetectedRecommended / totalRecommended\n",
    "    print(\"Accuracy is : \", accuracy)\n",
    "    print(\"Precision is : \", precision)\n",
    "    print(\"Recall is : \", recall)\n",
    "    print(\"F1 is : \", 2 * precision * recall / (precision + recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "هر کدام از این دو به تنهایی مفید نخواهد بود؛ زیرا در خیلی از موارد، ما مدل‌هایی با اندازه‌ی کلاس‌های برابر نداشته باشیم. مثلا ۹۹ درصد داده‌هایمان از یک کلاس باشند. آنگاه اگر کدمان همیشه کلاس ۹۹ درصدی را خروجی دهد، کار خاصی نکرده است. پس باید معیار دیگه‌ای را انتخاب کنیم.\n",
    "\n",
    "برای مثال:\n",
    "\n",
    "فرض کنید الگوریتم ما برای نصفی از حالات پیشنهاد شده و نشده درست پیش‌بینی می‌کند.\n",
    "اگر محصول ما خوب باشد، پس کاربران زیادی پیش‌نهادش می‌کنند و فقط درصد کمی هستند که پیش‌نهادش نمی‌کنند. پس طبق فرمول، پرسیشن بالایی داریم و ریکال کمی.\n",
    "\n",
    "همچنین اگر محصول ما محصول خوبی نباشد، طبق فرمول می‌رسیم به این که ریکال خوبی داریم و پرسیشن کمی. با این که الگوریتم ما کار مهمی نکرده و نصفی از داده‌های هر کلاس را اشتباه می‌گوید."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "از این تابع استفاده می‌کنیم زیرا با مقادیر خیلی بزرگ مقابله می‌کند که اتفاقا این مقادیر بزرگ همان‌طور که در مثال بالا گفته شد، بسیار اتفاق می‌افتد. مثلا یکی از تابع‌ها نزدیک یک باشد و دیگری نزدیک ۰. اگر ما میانگین را اعمال کنیم به درصد ۵۰ می‌رسیم. اما در این فرمول ما هنوز هم اف‌یک کوچکی خواهیم داشت."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7\n",
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Wrong samples: \n",
      "title                                               [وری, گود]\n",
      "comment      [تازه, خریدم, یه, مدت, کار, بکنه, مشخص, میشه, ...\n",
      "recommend                                          recommended\n",
      "Name: 0, dtype: object\n",
      "title                                      [نقد, پس, از, خرید]\n",
      "comment      [سلام, ،, راحت, شدم, از, کابل, شارژ, ،, توصیه,...\n",
      "recommend                                          recommended\n",
      "Name: 8, dtype: object\n",
      "title                                                 [mi, ۴w]\n",
      "comment      [کاور, مقاوم, و, قشنگیه, اما, متأسفانه, مدت, ز...\n",
      "recommend                                      not_recommended\n",
      "Name: 12, dtype: object\n",
      "title                                           [گزینه, خوبیه]\n",
      "comment      [سلام, ،, گزینه, خوبیه, من, یه, mg, ۲۵۴۰, خرید...\n",
      "recommend                                          recommended\n",
      "Name: 15, dtype: object\n",
      "title                              [تستر, پرادا, کندی, فلورال]\n",
      "comment      [من, این, ادو, تویلتو, خریدم, بر, خلاف, اینکه,...\n",
      "recommend                                          recommended\n",
      "Name: 17, dtype: object\n",
      "Accuracy is :  0.88875\n",
      "Precision is :  0.9307479224376731\n",
      "Recall is :  0.84\n",
      "F1 is :  0.883048620236531\n"
     ]
    }
   ],
   "source": [
    "testAndPrint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.88375\n",
      "Precision is :  0.9299719887955182\n",
      "Recall is :  0.83\n",
      "F1 is :  0.8771466314398944\n"
     ]
    }
   ],
   "source": [
    "testAndPrint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.855\n",
      "Precision is :  0.94375\n",
      "Recall is :  0.755\n",
      "F1 is :  0.8388888888888888\n"
     ]
    }
   ],
   "source": [
    "testAndPrint(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  0.845\n",
      "Precision is :  0.9509803921568627\n",
      "Recall is :  0.7275\n",
      "F1 is :  0.8243626062322947\n"
     ]
    }
   ],
   "source": [
    "testAndPrint(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algorithm | Accuracy | Precision | Recall | F1\n",
    "| --- | --- | --- | --- | --- |\n",
    "| a | 0.88875 | 0.9307479224376731 | 0.84 | 0.883048620236531\n",
    "| b | 0.88375 | 0.9299719887955182 | 0.83 | 0.8771466314398944\n",
    "| c | 0.855 | 0.94375 | 0.755 | 0.8388888888888888\n",
    "| d | 0.845 | 0.9509803921568627 | 0.7275 | 0.8243626062322947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8\n",
    "در حالت استفاده از هر دوی این حالات، نتیجه بسیار مطلوب هست همان‌طور که انتظار داشتیم. اگر هیچ‌یک را استفاده نکنیم نیز بدترین حالت خواهد بود. نکته‌ی مهم دیگر این است که تاثیر اسموتینگ مقداری بیش‌تر از نرمالایز کردن نوشته‌هاست که منطقی است زیرا تابع عالی‌ای برای این کار نوشته نشده است.\n",
    "\n",
    "نکته‌ی مهم دیگر تفاوت پرسیشن و ریکال در نبودن سموفینگ است. که انتظار هم می‌رفت زیرا کلماتی که در وجود ندارند، همان‌طور که توضیح داده شد، باعث می‌شوند احتمال کلاسی صفر شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "کلاس‌های غلط حدس زده شده:\n",
    "\n",
    "title                                               [وری, گود]\n",
    "\n",
    "comment      [تازه, خریدم, یه, مدت, کار, بکنه, مشخص, میشه, ...\n",
    "\n",
    "recommend                                          recommended\n",
    "\n",
    "title                                      [نقد, پس, از, خرید]\n",
    "\n",
    "comment      [سلام, ،, راحت, شدم, از, کابل, شارژ, ،, توصیه,...\n",
    "\n",
    "recommend                                          recommended\n",
    "\n",
    "title                                                 [mi, 4w]\n",
    "\n",
    "comment      [کاور, مقاوم, و, قشنگیه, اما, متأسفانه, مدت, ز...\n",
    "\n",
    "recommend                                      not_recommended\n",
    "\n",
    "title                                           [گزینه, خوبیه]\n",
    "\n",
    "comment      [سلام, ،, گزینه, خوبیه, من, یه, mg, 2540, خرید...\n",
    "\n",
    "recommend                                          recommended\n",
    "\n",
    "title                              [تستر, پرادا, کندی, فلورال]\n",
    "\n",
    "comment      [من, این, ادو, تویلتو, خریدم, \\r, بر, خلاف, ای...\n",
    "\n",
    "recommend                                          recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# راهکار‌های بهتر شدن اگوریتم و ادامه‌ی ۹\n",
    "دلیل اشتباه در این تست‌ها به نظرم می‌تواند مشکل در تعداد کم بودن داده‌های آموزش باشد که الگوریتم با کلمه‌ی گود آشنا نباشد. می‌تواند بد بودن پیش‌پردازشمان باشد که ویرگول و بک‌اسلش آر را از این لیست‌ها حذف نکرده است یا مثلا نتوانسته به خوبی، خوبیه رو به ریشه‌ی خوب تبدیل کنه."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
